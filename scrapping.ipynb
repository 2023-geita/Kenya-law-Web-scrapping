{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import numpy as np\n",
    "import re\n",
    "import requests\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the WebDriver\n",
    "driver = webdriver.Firefox()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intializing the base url\n",
    "base_url = \"https://new.kenyalaw.org\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_page(url):\n",
    "    driver.get(url)\n",
    "    WebDriverWait(driver, 3).until(EC.presence_of_element_located((By.TAG_NAME, 'body')))\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    return driver, soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing_url = \"https://new.kenyalaw.org/judgments/court-class/superior-courts/\"\n",
    "# page_driver, page = scrape_page(testing_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# page_driver.title\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def years_links_extract(url, page):\n",
    "    ul_element = page.find(\"ul\", class_=\"year-nav mb-0 ms-2\")\n",
    "    years_links = []\n",
    "    if ul_element.findAll(\"li\"):\n",
    "        for li in ul_element.findAll(\"li\"):\n",
    "            a_tag = li.find(\"a\")\n",
    "            if a_tag and \"href\" in a_tag.attrs:\n",
    "                years_links.append(url + a_tag[\"href\"].split(\"/\")[-2] + \"/\")\n",
    "    return years_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# years = years_links_extract(testing_url, page)\n",
    "# years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import urljoin\n",
    "\n",
    "def months_links_extract(url, page):\n",
    "    ul_elements = page.find_all(\"ul\", class_=\"year-nav mb-0 ms-2\")\n",
    "    \n",
    "    # Ensure at least two elements exist before accessing the second\n",
    "    if len(ul_elements) < 2:\n",
    "        print(f\"Warning: Expected at least 2 'ul' elements, found {len(ul_elements)} on page: {url}\")\n",
    "        return []  # Return empty list if structure doesn't match\n",
    "    \n",
    "    ul_element = ul_elements[1]\n",
    "    months_links = []\n",
    "    \n",
    "    # Extract month links from the `ul_element`\n",
    "    for li_tag in ul_element.find_all(\"li\"):\n",
    "        a_tag = li_tag.find(\"a\")\n",
    "        if a_tag and a_tag.get(\"href\"):\n",
    "            # Ensure URLs are fully qualified\n",
    "            full_link = urljoin(url, a_tag[\"href\"])\n",
    "            months_links.append(full_link)\n",
    "    \n",
    "    return months_links\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing_url2 = \"https://new.kenyalaw.org/judgments/court-class/superior-courts/2019/\"\n",
    "# page_driver2, page2 = scrape_page(testing_url2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# months_links_extract(testing_url2, page2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing_url3 = \"https://new.kenyalaw.org/judgments/court-class/superior-courts/2019/1\"\n",
    "# page_driver3, page3 = scrape_page(testing_url3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Extracting page numbers\n",
    "def extract_page_numbers_links(url, page):\n",
    "    ul_element = page.find(\"ul\", class_=\"pagination flex-wrap\")\n",
    "    page_numbers = []\n",
    "    if ul_element:\n",
    "        for li in ul_element.findAll(\"li\"):\n",
    "            a_tag = li.find(\"a\")\n",
    "            if a_tag and \"href\" in a_tag.attrs:\n",
    "                page_numbers.append(f\"{url}&{a_tag['href'][12:]}\")\n",
    "    return list(set(page_numbers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def exctract_alphabetical_links(url):\n",
    "    alphabets = [chr(i) for i in range(ord('a'), ord('z') + 1)]\n",
    "    links = []\n",
    "    for alphabet in alphabets:\n",
    "        link = f\"{url}?alphabet={alphabet}\"\n",
    "        links.append(link)\n",
    "    return links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdf_links(page):\n",
    "    tr_elements = page.find_all(\"tr\")\n",
    "    links = []\n",
    "    for tr in tr_elements:\n",
    "        td_title = tr.find(\"td\", class_=\"cell-title\")\n",
    "        if td_title:\n",
    "            a_tag = td_title.find(\"a\")\n",
    "            if a_tag and \"href\" in a_tag.attrs:\n",
    "                links.append(base_url + a_tag[\"href\"])\n",
    "    return links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_pdf_size_greater_than_zero(text):\n",
    "    match = re.search(r'(\\d+(\\.\\d+)?)\\s*KB', text)\n",
    "    if match:\n",
    "        size_in_kb = float(match.group(1))\n",
    "        return size_in_kb > 0\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def extract_pdf_link(url):\n",
    "    driver.get(url)\n",
    "    pdf_download_page_driver, pdf_download_page = scrape_page(url)\n",
    "    a_element = pdf_download_page_driver.find_element(By.CSS_SELECTOR, \"a.btn.btn-primary.btn-shrink-sm\")\n",
    "    a_tag = a_element.get_attribute(\"href\")\n",
    "    pdf_size_greater_than_zero = is_pdf_size_greater_than_zero(a_element.text.strip())\n",
    "    \n",
    "    if pdf_size_greater_than_zero:\n",
    "        return a_tag\n",
    "    else:\n",
    "        dd_elements = pdf_download_page.find_all(\"dd\") \n",
    "        a_tag = dd_elements[-1].find(\"a\")\n",
    "        download_link = a_tag[\"href\"]\n",
    "        return download_link\n",
    "    \n",
    "# extract_pdf_link(\"https://new.kenyalaw.org/akn/ke/judgment/kehc/1990/87/eng@1990-12-19\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exctract_all_cases_links_in_a_query(url):\n",
    "    all_alphabets_links = exctract_alphabetical_links(url)\n",
    "    all_pdfs = []\n",
    "    for alphabet_link in all_alphabets_links:\n",
    "        page_1_driver, page_1 = scrape_page(alphabet_link)\n",
    "        pages_links = extract_page_numbers_links(alphabet_link, page_1)\n",
    "        \n",
    "        for page_link in pages_links:           \n",
    "            page_2_driver, page_2 = scrape_page(page_link)\n",
    "            pdf_download_page_links = pdf_links(page_2)\n",
    "            for link in pdf_download_page_links:\n",
    "                pdf_link = extract_pdf_link(link)\n",
    "                all_pdfs.append(pdf_link)\n",
    "    \n",
    "    print(f\"Total PDFs found: {len(all_pdfs)}\")\n",
    "    return np.array(all_pdfs).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_files(url_list, folder_path):\n",
    "    if not os.path.exists(folder_path):\n",
    "        os.makedirs(folder_path)\n",
    "\n",
    "    total_files = len(url_list)\n",
    "    downloaded_files = 0\n",
    "    failed_downloads = []\n",
    "\n",
    "    print(f\"Starting download of {total_files} files.\")\n",
    "\n",
    "    for url in url_list:\n",
    "        print(f\"Preparing to download: {url}\")\n",
    "        # Extract a valid filename from the URL, replacing unwanted parts\n",
    "        filename = os.path.join(folder_path, url.split('/')[-1].replace(\"source\", \"\").replace(\" \", \"_\") + \".pdf\")\n",
    "        \n",
    "        # Download the file and check success\n",
    "        success = download_file(url, filename)\n",
    "        if success:\n",
    "            downloaded_files += 1\n",
    "        else:\n",
    "            failed_downloads.append(url)\n",
    "\n",
    "        print(f\"Files remaining: {total_files - downloaded_files}\")\n",
    "\n",
    "    # Attempt to redownload failed files\n",
    "    if failed_downloads:\n",
    "        print(f\"\\nAttempting to redownload {len(failed_downloads)} failed files...\")\n",
    "        for url in failed_downloads[:]:  # Create a copy to iterate over\n",
    "            filename = os.path.join(folder_path, url.split('/')[-1].replace(\"source\", \"\").replace(\" \", \"_\") + \".pdf\")\n",
    "            if download_file(url, filename):\n",
    "                downloaded_files += 1\n",
    "                failed_downloads.remove(url)\n",
    "\n",
    "    print(f\"\\nDownload complete. {downloaded_files} files downloaded successfully.\")\n",
    "    if failed_downloads:\n",
    "        print(f\"Failed to download {len(failed_downloads)} files:\")\n",
    "        for url in failed_downloads:\n",
    "            print(url)\n",
    "\n",
    "def download_file(url, filename):\n",
    "    try:\n",
    "        print(f\"Downloading: {url}\")  # Log the URL being downloaded\n",
    "        response = requests.get(url, timeout=30)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            with open(filename, 'wb') as file:\n",
    "                file.write(response.content)\n",
    "            print(f\"Successfully downloaded: {filename}\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"Failed to download {url}: Status code {response.status_code}\")\n",
    "            return False\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Error downloading {url}: {e}\")\n",
    "        return False\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_page_scrapper(url):\n",
    "    all_downloadable_links = set([])\n",
    "\n",
    "    # Scrape the main page\n",
    "    scraped_page_driver, scraped_page = scrape_page(url)\n",
    "\n",
    "    # Year links\n",
    "    years_links = years_links_extract(url, scraped_page)\n",
    "    print(\"****************Got year links****************\")\n",
    "\n",
    "    for year_link in years_links:\n",
    "        year_page_driver, year_page = scrape_page(year_link)\n",
    "\n",
    "        # Month links\n",
    "        months_links = months_links_extract(year_link, year_page)\n",
    "        print(\"****************Got month links****************\")\n",
    "\n",
    "        for month_link in months_links:\n",
    "            # Extract all the case links for the current month\n",
    "            downloadable_links = exctract_all_cases_links_in_a_query(month_link)\n",
    "            all_downloadable_links.update(np.array(downloadable_links).flatten())\n",
    "            \n",
    "            # Download all PDFs immediately after scraping month links\n",
    "            download_folder = \"downloaded_files\"  # Define your download folder\n",
    "            download_files(downloadable_links, download_folder)  # Call the download function\n",
    "\n",
    "    return all_downloadable_links\n",
    "\n",
    "# Call the function to start scraping\n",
    "final_page_scrapper(\"https://new.kenyalaw.org/judgments/KESC/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.close()\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
